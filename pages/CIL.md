- knowledge distillation #card
  card-last-interval:: 4
  card-repeats:: 1
  card-ease-factor:: 2.36
  card-next-schedule:: 2024-11-25T08:58:19.616Z
  card-last-reviewed:: 2024-11-21T08:58:19.617Z
  card-last-score:: 3
	- {{cloze A technique which transfers knowledge from a complex model (the teacher) to a simpler model (the student) while minimizing the forgetting of previously learned information. E.g: Mixture of Experts exploit **frozen CLIP as teacher model** and then use its output features as input for a smaller model, the small one is a student model }}
-
-